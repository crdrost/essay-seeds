In 1950, a man named Alan Turing proposed to investigate the question "Can Machines Think?" in an article in the magazine *Mind*, and immediately jumped aside the questions of what 'thinking' consists in by providing what we would now call a "duck typing" answer, after James Whitcomb Riley's famous quote,
    
    "When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck."

Turing proposed an "imitation game" where a single person -- an interrogator -- was able to communicate with a human and a robot: the human is trying to prove that he or she is not a robot, while the robot is trying to prove that he or she is a human. Turing also suggests that he thinks the best strategies are to act naturally, but he does not attempt a modification of the strategy to enforce this -- e.g. by not telling the human that they are trying to be distinguished, as human, from a robot.

This imitation game is today called a Turing test. He didn't say exactly how long the test was to go on for, but he did guess that by the year 2000, a 5-minute imitation game could fool 30% of interrogators. What does that have to do with machines thinking? Turing says, 'The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.' 

In other words, if it acts this way, then the question of whether it is, in essence, actually *thinking* is of no consequence. It quacks like a duck, so we will call it a duck. It is a 'duck typed' approach to thinking in terms of conversational ability.

The more I think about it, the more profoundly peculiar the Turing test seems.

I guess the first point to be made is that 5 minutes is not long enough for any meaningful treatment of the question, and I can give a very simple reason why. The Turing test is dealing with a particular case of "expertise" on the part of humans -- we are social animals who have 20+ years of expertise in talking with other humans. Thus the duck-typing in general might be phrased as "If you can fool an expert for 5 minutes, then we will call you an expert." 

I can't think of any field where this would be a realistic criterion. Heck, science has had folks who have "fooled experts" for years before their lack of expertise was pointed out. In any case, computers are so much more ubiquitous than Turing ever predicted that an android *would* have to keep up a pretense of being a functioning member of human society for some years before we really felt that we were in a gray area of saying "of course it can't think." 

The next point to be made is that we are assuming that being an expert makes one qualified to judge the expertise of others. This is reasonable when "expert" attends all of the conscious facilities that we're already accustomed to, but what if an "expert" doesn't have those? 

means "expert", but it might not be reasonable if we go further along. Imagine that we do indeed have a machine that we would say "thinks" -- is it therefore also a good judge of whether other machines "think"? 

If it were a crummy job, perhaps that would be an indicator that it didn't really "think" -- in other words, perhaps this forms a higher-order Turing test. On the other hand, if a good efficient Turing test takes time T, then this Turing test takes time 10 T, if we ask it to run 10 Turing Tests serially. I think this is why we don't in practice require it when giving people degrees.

In some sense I suppose there is a higher-order Turing test here. Imagine that we invite this computer to classify humans and computers as "thinking" or "nonthinking" in a Turing test, and we have another human doing the same. We now ask a human whether they can tell the difference between the human and the computer. 



Notice that this is distinct from the original Turing test in part because of time constraints. That is, you might see that we could actually perform this Turing test *within* the original Turing test, asking both the human and the computer to categorize discussion partners as machines or people. But if a good Turing test requires a time at least T, and we require both human and machine to match on their judgments of 10 people, then this higher-order Turing test requires a time 10T.  

This computer will label other humans and machines as "thinking" or "nonthinking," and a human does the same thing in another room, and we 

The first thing to notice is that we can probably make the game easier for the robot by not telling the human that they are playing the game -- for we have no reason to expect a human to do *worse* when they know that they're trying to distinguish themselves from a robot. 

